{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** Ecrire la vraisemblance $\\mathcal{L}(\\beta)$ du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a par hypothèse que: $X|Y \\hookrightarrow \\mathcal{B}(\\pi(X))$.\n",
    "\n",
    "La vraisemblance de $n$ variables aléatoires de Bernoulli peut donc être exprimée ainsi: \n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\prod\\limits_{i=1}^n \\mathbb{P}(Y=y_i | X = \\bm{x}_i) = \\prod\\limits_{i=1}^n \\pi(\\bm{x}_i)^{y_i}(1-\\pi(\\bm{x}_i))^{1-y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Montrer que l'estimateur du maximum de vraisemblance peut s'obtenir en considérant l'algorithme itératif suivant:\n",
    "\n",
    "$$\\beta^{(s+1)} = \\beta^{(s)} + (X^TV^{(s)}X)^{-1}X^T(\\bm{y}-\\bm{\\pi}^{(s)})$$\n",
    "\n",
    "où:\n",
    "\n",
    "- $X$ est la matrice formée d’une première colonne de coordonnées constantes\n",
    "égales à 1 et des $p$ colonnes correspondant aux variables $X_1, ...,X_p$ observées sur les $n$ individus.\n",
    "- $\\bm{y} = (y_1 ... y_n)^T$ ets le vecteur colonne de labels associés à chaque $x_i$\n",
    "- $\\bm{\\pi}^{(s)}$ est le vecteur formé des $\\pi_i = \\pi(x_i)$ estimé à l'itération courante $s$.\n",
    "- $V^{(s)}$ est la matrice diagonale formée des $\\pi^{(s)}(1 - \\pi^{(s)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la question précédente, on obtient la log-vraisemblance:\n",
    "\n",
    "$$\\mathcal{l}(\\beta) = \\sum_{i=1}^n y_i \\log(\\pi(\\bm{x_i})) + (1 - y_i) \\log(1 - \\pi(\\bm{x_i}))$$ \n",
    "\n",
    "i.e.\n",
    "$$\\mathcal{l}(\\beta) = \\sum_{i=1}^n y_i (\\bm{x_i}^T\\beta - \\log(1 + e^{\\bm{x_i}^T\\beta})) - (1 - y_i) \\log(1 + e^{\\bm{x_i}^T\\beta})$$\n",
    "\n",
    "Ainsi:\n",
    "\n",
    "$$\\mathcal{l}(\\beta) = \\sum\\limits_{i=1}^n y_i \\bm{x}_i^T\\beta - \\log(1+e^{\\bm{x}_i^T\\beta })$$\n",
    "\n",
    "En prenant le gradient selon $\\beta$:\n",
    "\n",
    "$$\\nabla_\\beta \\mathcal{l}(\\beta) = \\sum\\limits_{i=1}^n y_i\\bm{x}_i - \\frac{\\bm{x_i}e^{\\bm{x}_i^T \\beta}}{1 + e^{\\bm{x}_i^T \\beta}} = \\sum\\limits_{i=1}^n (y_i-\\pi(\\bm{x}_i))\\bm{x_i} = X^T(\\bm{y} - \\bm{\\pi})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en déduit la hessienne de la log-vraisemblance:\n",
    "\n",
    "$$\\forall j \\in [\\![1, p]\\!], \\nabla_\\beta (\\nabla_\\beta l(\\beta)_j) = \\nabla_\\beta \\sum\\limits_{i=1}^n [y_i - \\pi(x_i)]x_{ij} = \\sum\\limits_{i=1}^n \\nabla_\\beta [y_i - \\pi(x_i)]x_{ij} = - \\sum\\limits_{i=1}^n \\nabla_\\beta  [\\pi(x_i)x_{ij}]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e.\n",
    "\n",
    "$$\\forall j \\in [\\![1, p]\\!], \\nabla_\\beta (\\nabla_\\beta l(\\beta)_j) = -\\sum\\limits_{i=1}^n \\nabla_\\beta \\left[\\frac{1}{1 + e^{-\\bm{x}_i^T\\beta}} \\right] x_{ij} = -\\sum\\limits_{i=1}^n \\left[\\frac{1}{1 + e^{-\\bm{x}_i^T\\beta}} \\right]^2 x_{ij} e^{-\\bm{x}_i^T\\beta} \\bm{x}_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où:\n",
    "\n",
    "$$\\forall j \\in [\\![1, p]\\!], \\nabla_\\beta (\\nabla_\\beta l(\\beta)_j) = -\\sum\\limits_{i=1}^n \\left[\\frac{e^{-\\bm{x}_i^T\\beta}}{1 + e^{-\\bm{x}_i^T\\beta}} \\right] \\left[\\frac{1}{1 + e^{-\\bm{x}_i^T\\beta}} \\right] x_{ij} \\bm{x}_i = -\\sum\\limits_{i=1}^n [1 - \\pi(x_i)]\\pi(x_i) x_{ij} \\bm{x}_i$$\n",
    "\n",
    "Ainsi:\n",
    "\n",
    "$$\\nabla_\\beta (\\nabla_\\beta l(\\beta)) = -\\sum\\limits_{i=1}^n [1 - \\pi(x_i)]\\pi(x_i) \\bm{x}_i \\bm{x}_i^T = -X^TVX$$\n",
    "\n",
    "avec:\n",
    "\n",
    "$$V=\\text{diag}((\\pi(x_i)[1 - \\pi(x_i)])_{1\\leq i\\leq n})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se place à l'étape $s$ de notre algorithme.\n",
    "Le développement en série de Taylor de $\\mathcal{l}(\\beta)$ à l'ordre 2 en un point proche de $\\beta^{(s)}$ s'écrit:\n",
    "\n",
    "$$\\mathcal{l}(\\beta) = \\mathcal{l}(\\beta^{(s)}) + \\nabla_\\beta\\mathcal{l}(\\beta^{(s)})^T(\\beta - \\beta^{(s)}) + \\frac{1}{2}(\\beta - \\beta^{(s)})^T\\nabla_\\beta (\\nabla_\\beta l(\\beta^{(s)}))(\\beta - \\beta^{(s)}) + o(\\lVert \\beta - \\beta^{(s)}\\rVert^2)$$\n",
    "\n",
    "Dans la suite, on ignore le terme négligeable de l'expression ci-dessus. Afin de maximiser la log-vraisemblance, on choisit un $\\beta^{(s+1)}$ qui maximise cette approximation.\n",
    "\n",
    "Pour se faire, on dérive $\\mathcal{l}(\\beta)$ selon $\\beta$, et on regarde quand la dérivée s'annule. On obtient alors la relation suivante:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal{l}(\\beta^{(s)}) + \\nabla_\\beta (\\nabla_\\beta l(\\beta^{(s)}))(\\beta^{(s+1)} - \\beta^{(s)}) = 0$$\n",
    "\n",
    "On obtient donc la relation suivante entre $\\beta^{(s)}$ et $\\beta^{(s+1)}$:\n",
    "\n",
    "$$\\beta^{(s+1)} = \\beta^{(s)} - \\nabla_\\beta (\\nabla_\\beta l(\\beta^{(s)}))^{-1}\\nabla_\\beta\\mathcal{l}(\\beta^{(s)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En replaçant l'expression de la hessienne et du gradient dans cette expression avec leur formes matricielles identifiées plus haut, on obtient:\n",
    "\n",
    "$$\\beta^{(s+1)} = \\beta^{(s)} + (X^TV^{(s)}X)^{-1}X^T(\\bm{y} - \\bm{\\pi}^{(s)})$$ \n",
    "i.e le résultat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** En remarquant que $\\pi_i(1 - \\pi_i)$ est majoré par $\\frac{1}{4}$, proposer une approximation $H_2$ de $H_1$ telle que $H_1 - H_2$ soit définie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme le fait remarquer l'énoncé,\n",
    "\n",
    "$$f: x \\mapsto x(1-x)$$\n",
    "\n",
    "admet un maximum sur $[0, 1]$ en $\\frac{1}{2}$, et y prend la valeur $\\frac{1}{4}$. On a alors:\n",
    "\n",
    "$$ \\forall x \\in [0, 1], f(x) \\le \\frac{1}{4}$$\n",
    "\n",
    "En particulier, \n",
    "\n",
    "$$\\forall s \\in \\mathbb{N}, \\forall i \\in [\\![1, n]\\!], \\pi_i^{(s)}(1-\\pi_i^{(s)}) \\le \\frac{1}{4}$$\n",
    "\n",
    "Soit $\\varepsilon > 0$ fixé. Considérons la matrice:\n",
    "\n",
    "$$H_2 = -(\\frac{1}{4} + \\varepsilon)X^TX$$\n",
    "\n",
    "Soit $v \\in \\mathbb{R}^p \\setminus \\{0\\}$ fixé quelconque. On a:\n",
    "\n",
    "$$v^T(H_1 - H_2)v = (Xv)^T((\\frac{1}{4} + \\varepsilon)I_n - V^{(s)})(Xv) = \\sum_{i=1}^n ((\\frac{1}{4} + \\varepsilon) - \\pi_i^{(s)}(1-\\pi_i^{(s)})) (Xv)_j^2 \\ge \\varepsilon \\lVert Xv \\rVert_2^2 > 0$$\n",
    "\n",
    "sous l'hypothèse que $X$ est injective.\n",
    "\n",
    "Avec ce choix de $H_2$, on a bien $H_1 - H_2$ définie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Réécrire l'algorithme itératif en y injectant cette approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le choix de $H_2$ ci-dessus, et en prenant $\\varepsilon$ suffisamment petit, on peut réécrire l'algorithme obtenu à la question 2 sous la forme suivante:\n",
    "\n",
    "$$\\beta^{(s+1)} =  \\beta^{(s)} + 4(X^TX)^{-1}X^T(\\bm{y} - \\bm{\\pi^{(s)}})$$\n",
    "\n",
    "où nous sommes passés à la limite lorsque $\\varepsilon \\rightarrow 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Discuter l'intérêt de considérer la maximisation de la vraisemblance pénalisée.\n",
    "\n",
    "Il s'agit d'un terme de pénalité qui resemble à la pénalité Ridge, avec une norme $\\mathcal{l}_2$. Ceci a pour effet de faire du shrinkage sur la solution obtenue, à savoir borner les coefficients du $\\beta$ maximisant la vraisemblance. Cette estimateur sera meilleur en prédiction tous en étant plus sparse.\n",
    "\n",
    "De plus, le $\\lambda$, si bien choisi, permet de contourner des problèmes d'inversibilité de la matrice $X^TX$ en ajoutant un terme en $\\lambda I_p$. Ces problèmes d'inversibilité surviennent notamment lorsque:\n",
    "- Les colonnes de X sont liées (certaines variables explicatives sont corrélées)\n",
    "- Lorsque l'on a plus de variables explicatives que d'observations en grande dimension ($n < p$), car $rang(X^TX) \\le n$\n",
    "\n",
    "- En grande dimension, pour peu d'observations, le terme rend la matrice définie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6.** Montrer que la maximisation de la vraisemblance pénalisée peut s'obtenir en considérant l'algorithme suivant:\n",
    "\n",
    "$$\\beta_{\\lambda}^{(s+1)} = \\beta_{\\lambda}^{(s)} + 4(X^TX + 4\\lambda I_p)^{-1}(X^T(\\bm{y} - \\bm{\\pi}) - \\lambda\\beta_{\\lambda}^{(s)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par définition, l'expression de la vraisemblance pénalisée s'exprime:\n",
    "\n",
    "$$\\mathcal{L}_{\\lambda}(\\beta) = \\mathcal{L}(\\beta) - \\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$$\n",
    "\n",
    "Or maximiser cette expression revient à maximiser:\n",
    "\n",
    "$$\\mathcal{l}_{\\lambda}(\\beta) = \\mathcal{l}(\\beta) - \\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en déduit des expressions obtenues à la question 2 pour le gradient et la hessienne de $\\mathcal{l}(\\beta)$ celles de $\\mathcal{l}_{\\lambda}(\\beta)$:\n",
    "\n",
    "$$\\nabla_\\beta\\mathcal{l}_{\\lambda}(\\beta) = \\nabla_\\beta\\mathcal{l}(\\beta) - \\frac{\\lambda}{2} \\nabla_\\beta(\\lVert \\beta \\rVert_2^2) = X^T(\\bm{y} - \\bm{\\pi}) - \\lambda\\beta$$\n",
    "\n",
    "et:\n",
    "\n",
    "$$\\nabla_\\beta (\\nabla_\\beta l_{\\lambda}(\\beta)) = -X^TVX -\\lambda I_p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En se plaçant à l'étape $s$, et en effectuant un développement de Taylor à l'ordre 2 comme à la question 2 en un $\\beta$ proche de $\\beta_{\\lambda}^{(s)}$, on obtient:\n",
    "\n",
    "$$\\mathcal{l}_{\\lambda}(\\beta) = \\mathcal{l}_{\\lambda}(\\beta_{\\lambda}^{(s)}) + \\nabla_\\beta\\mathcal{l}_{\\lambda}(\\beta^{(s)})^T(\\beta - \\beta^{(s)}) + \\frac{1}{2}(\\beta - \\beta^{(s)})^T\\nabla_\\beta (\\nabla_\\beta \\mathcal{l}_{\\lambda}(\\beta^{(s)}))(\\beta - \\beta^{(s)}) + o(\\lVert \\beta - \\beta^{(s)}\\rVert^2)$$\n",
    "\n",
    "Dans la suite, on ignore le terme négligeable de l'expression ci-dessus. Afin de maximiser la log-vraisemblance, on choisit un $\\beta_{\\lambda}^{(s+1)}$ qui maximise cette approximation.\n",
    "\n",
    "Pour se faire, on dérive $\\mathcal{l}_{\\lambda}(\\beta)$ selon $\\beta$, et on regarde quand la dérivée s'annule. On obtient alors la relation suivante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla_\\beta\\mathcal{l}_{\\lambda}(\\beta_{\\lambda}^{(s)}) + \\nabla_\\beta (\\nabla_\\beta l_{\\lambda}(\\beta_{\\lambda}^{(s)}))(\\beta_{\\lambda}^{(s+1)} - \\beta_{\\lambda}^{(s)}) = 0$$\n",
    "\n",
    "et donc:\n",
    "\n",
    "$$\\beta_{\\lambda}^{(s+1)} = \\beta_{\\lambda}^{(s)} - \\nabla_\\beta (\\nabla_\\beta l_{\\lambda}(\\beta_{\\lambda}^{(s)}))^{-1}\\nabla_\\beta\\mathcal{l}_{\\lambda}(\\beta_{\\lambda}^{(s)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En remplaçant les expressions du gradient et de la hessienne de $\\mathcal{l}_{\\lambda}(\\beta)$ obtenues ci-dessus, on obtient alors:\n",
    "\n",
    "$$\\beta_{\\lambda}^{(s+1)} = \\beta_{\\lambda}^{(s)} + (X^TV^{(s)}X + \\lambda I_p)^{-1}(X^T(\\bm{y} - \\bm{\\pi}) - \\lambda \\beta_{\\lambda}^{(s)})$$\n",
    "\n",
    "En approximant $X^TV^{(s)}X$ par la matrice $H_2$ obtenue à la question 3, on obtient:\n",
    "\n",
    "$$\\beta_{\\lambda}^{(s+1)} = \\beta_{\\lambda}^{(s)} + (\\frac{1}{4}X^TX + \\lambda I_p)^{-1}(X^T(\\bm{y} - \\bm{\\pi}) - \\lambda \\beta_{\\lambda}^{(s)})$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\\beta_{\\lambda}^{(s+1)} = \\beta_{\\lambda}^{(s)} + 4(X^TX + 4\\lambda I_p)^{-1}(X^T(\\bm{y} - \\bm{\\pi}) - \\lambda \\beta_{\\lambda}^{(s)})$$\n",
    "\n",
    "ce qui correspond à l'expression voulue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.** Montrer que la maximisation de la vraisemblance pénalisée revient à résoudre le problème d'optimisation suivant:\n",
    "\n",
    "$$\\min\\limits_{\\beta\\in\\mathbb{R}^p} \\sum_{i=1}^{n} \\log(1 + e^{- \\overline y_i \\beta^Tx_i}) + \\frac{\\lambda}{2}\\lVert \\beta \\rVert_2^2$$\n",
    "\n",
    "où $\\overline y_i$ est la variable encodée en $\\{ -1, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximiser $\\mathcal{l}_{\\lambda}(\\beta)$  équivaut à minimiser $-\\mathcal{l}_{\\lambda}(\\beta)$ pour $\\beta\\in\\mathbb{R}^p$. Or:\n",
    "\n",
    "$$-\\mathcal{l}_{\\lambda}(\\beta) = \\sum\\limits_{i=1}^n (\\log(1+e^{\\beta^T \\bm{x}_i}) - y_i\\beta^T \\bm{x}_i) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2 = \\sum\\limits_{i=1}^n \\log(e^{-y_i\\beta^Tx_i}(1+e^{\\beta^T \\bm{x}_i})) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2$$\n",
    "\n",
    "D'où:\n",
    "\n",
    "$$-\\mathcal{l}_{\\lambda}(\\beta) = \\sum\\limits_{i=1}^n \\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit la variable $\\overline y_i = 2 y_i - 1\\in\\{-1,1\\}$ et on remarque que:\n",
    "\n",
    "$$\\begin{cases}\n",
    "\\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) = \\log(1+e^{\\beta^T \\bm{x}_i}) & y_i = 0 & \\overline y_i = -1\\\\\n",
    "\\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) = \\log(1+e^{-\\beta^T \\bm{x}_i}) & y_i = 1 & \\overline y_i = 1\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Ainsi:\n",
    "\n",
    "$$\\forall i \\in [\\![1, n]\\!], \\log(e^{-y_i\\beta^Tx_i}+e^{(1-y_i)\\beta^T \\bm{x}_i}) = \\log(1+e^{-\\overline y_i\\beta^T \\bm{x}_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où:\n",
    "\n",
    "$$-\\mathcal{l}_{\\lambda}(\\beta) = \\sum\\limits_{i=1}^n \\log(1+e^{-\\overline y_i\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2$$\n",
    "\n",
    "La maximisation de $\\mathcal{l}_{\\lambda}(\\beta)$ pour $\\beta\\in\\mathbb{R}^p$ revient donc à déterminer:\n",
    "\n",
    "$$\\min\\limits_{\\beta\\in\\mathbb{R}^p}\\sum\\limits_{i=1}^n \\log(1+e^{-\\overline y_i\\beta^T \\bm{x}_i}) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons:\n",
    "\n",
    "$$l(z) = \\log(1+\\exp(-z))$$\n",
    "\n",
    "Nous pouvons reformuler le problème d'optimisation de la question précédente sous les contraintes $\\\\ \\forall i \\in [\\![1, n]\\!], \\overline y_i \\beta^Tx_i  = z_i$\n",
    "\n",
    "Le lagrangien s'exprime donc:\n",
    "\n",
    "$$\\mathcal{L}(\\beta, \\alpha) = \\sum\\limits_{i=1}^n l(\\overline y_i \\beta^Tx_i) + \\frac{\\lambda}{2}\\|\\beta\\|_2^2 + \\sum\\limits_{i=1}^n\\alpha_i(\\overline y_i\\beta^Tx_i - z_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or on a:\n",
    "\n",
    "$$l^*(z) = \\sup\\limits_{x \\in \\mathbb{R}}(zx - \\log(1+\\exp(-x)))$$\n",
    "\n",
    "Le maximum est atteint pour $-z = \\frac{1}{1 + e^{-(-x)}} = \\sigma(-x)$, et donc en $x = - \\log(\\frac{-z}{1 + z})$. Ainsi, on a:\n",
    "\n",
    "$$l^*(z) = -z\\log(\\frac{-z}{1+z}) - \\log(1 + e^{-(-\\log(\\frac{-z}{1+z}))}) = -z\\log(-z) + z\\log(1+z) - \\log(1 + \\frac{-z}{1+z} )$$\n",
    "\n",
    "D'où:\n",
    "\n",
    "$$l^*(z) = -z\\log(-z) + z\\log(1+z) + \\log(1+z) = -z\\log(-z) + (1+z)\\log(1+z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En notant $g(x) = \\frac{\\lambda}{2}\\|x\\|_2^2$, on a:\n",
    "\n",
    "$$g^*(x) = \\frac{1}{2\\lambda}\\|\\beta\\|_2^2$$\n",
    "\n",
    "\n",
    "Ainsi, puisque notre problème initial s'écrit:\n",
    "\n",
    "$$\\min\\limits_{\\beta\\in\\mathbb{R}^p}\\sum\\limits_{i=1}^n l(\\overline y_i\\beta^T \\bm{x}_i) + g(\\beta)$$\n",
    "\n",
    "son dual s'écrit:\n",
    "\n",
    "$$\\max\\limits_{\\alpha \\in \\mathbb{R}^n}\\sum\\limits_{i=1}^n -l^*(\\alpha_i) - g^*(-\\text{diag}(\\overline y)X^T \\alpha)$$\n",
    "\n",
    "i.e:\n",
    "\n",
    "$$\\max\\limits_{\\alpha\\in\\mathbb{R}^n}\\sum\\limits_{i=1}^n -\\alpha_i\\log(\\alpha_i) - \\left(1-\\alpha_i\\right)\\log\\left(1-\\alpha_i\\right) - \\frac{1}{2\\lambda}\\alpha^T\\text{diag}(\\overline y)X X^T\\text{diag}(\\overline y)\\alpha$$\n",
    "\n",
    "On notera:\n",
    "\n",
    "$$K:=\\text{diag}(\\overline y)X X^T\\text{diag}(\\overline y)$$\n",
    "\n",
    "Et:\n",
    "\n",
    "$$\\mathcal{D}(\\alpha) = \\sum\\limits_{i=1}^n -\\alpha_i\\log(\\alpha_i) - \\left(1-\\alpha_i\\right)\\log\\left(1-\\alpha_i\\right) - \\frac{1}{2\\lambda}\\alpha^TK\\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On en déduit l'expression du gradient et de la hessienne:\n",
    "\n",
    "$$\\nabla_{\\alpha} \\mathcal{D}(\\alpha)=-\\text{logit}(\\alpha) - \\frac{1}{\\lambda}K\\alpha$$\n",
    "\n",
    "et:\n",
    "\n",
    "$$\\nabla\\nabla_\\alpha \\mathcal{D}(\\alpha) = - \\text{diag}\\left(\\frac{1}{\\alpha(1-\\alpha)}\\right) - \\frac{1}{\\lambda}K\\simeq - 4I_n - \\frac{1}{\\lambda}K$$\n",
    "\n",
    "\n",
    "D'où:\n",
    "$$\\alpha^{(s+1)} = \\alpha^{(s)} + \\text{diag}(\\overline y)(4\\lambda I_n - XX^T)^{-1}(\\lambda\\text{diag}(\\overline y)\\text{logit}(\\alpha) + X X^T\\text{diag}(\\overline y)\\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratical part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put a linear SVM\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "A = pd.read_table(\"Alzheimer_Webster.txt\", header=0,sep=\" \")\n",
    "test_size=0.2\n",
    "target=\"Y\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put a linear SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "A = pd.read_table(\"Alzheimer_Webster.txt\", header=0,sep=\" \")\n",
    "test_size=0.2\n",
    "target=\"Y\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9.\n",
    " À l’aide du logiciel de votre choix, mettre en oeuvre la régression logistique régularisée sur le jeux de données Alzheimer. Si vous utiliser R,\n",
    "vous pouvez par exemple utiliser le package glmnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from  sklearn.linear_model import LogisticRegression\n",
    "# prepare data\n",
    "X = A.drop(target, axis=1)\n",
    "y = A[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss on the train set 2.220446049250313e-16\n",
      "accuracy on the train set 1.0\n",
      "Accuracy: 91.78%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "def question_9(penalty=\"l2\"):\n",
    "    model=LogisticRegression(penalty=penalty,max_iter=500)\n",
    "    #fit it \n",
    "    model.fit(X_train, y_train)\n",
    "    # check the result on the train set with the mean square error \n",
    "    y_pred=model.predict(X_train)\n",
    "    from sklearn.metrics import log_loss\n",
    "\n",
    "    print(\"log loss on the train set\",log_loss(y_train, y_pred))\n",
    "    # compute the accuracy \n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    print(\"accuracy on the train set\",accuracy)\n",
    "    #check the result\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "    return model,accuracy,y_pred\n",
    "model,accuracy,y_pred=question_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 \n",
    "Dans une boucle de validation croisée, entraîner une régression\n",
    "logistique régularisée. Tracer l’évolution du taux d’erreur de classification crossvalidé en fonction des hyper-paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all hyperparameters to swipe \n",
    "Cs=np.logspace(-2.3,2.3,10)\n",
    "fold=5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "def get_result_C(Cs,kernel,whole_dataset,target,fold,penalty=\"l2\"):\n",
    "    #train_dataset,test_dataset=train_test_split(A,test_size=test_size,random_state=42)\n",
    "    # create model\n",
    "    model=LogisticRegression(penalty=penalty,max_iter=1000,solver=\"liblinear\")\n",
    "    # actual training loop\n",
    "    train_scores,test_scores=validation_curve(estimator=model,X=whole_dataset.drop(columns=[target]),y=whole_dataset[target],param_name=\"C\",param_range=Cs,cv=fold,n_jobs=-1)\n",
    "\n",
    "    df=pd.DataFrame()\n",
    "    # fetch mean test and train score\n",
    "    df[\"train_score\"]=train_scores.mean(axis=1)\n",
    "    df[\"test_score\"]=test_scores.mean(axis=1)\n",
    "    df[\"C\"]=Cs\n",
    "    df[\"Kernel\"]=kernel\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def question_10(Cs,fold,penalty):   \n",
    "    # Compute for different C the models.\n",
    "    result_df=pd.DataFrame()\n",
    "    result_df_test=pd.DataFrame()\n",
    "    result_df=pd.concat([result_df,(get_result_C(Cs,\"linear\",whole_dataset=A,target=target,fold=5,penalty=penalty))])\n",
    "    import seaborn as sns\n",
    "    sns.lineplot(data=result_df, x=\"C\", y=\"train_score\",label=\"train\")\n",
    "    plt.xscale('log')\n",
    "    plt.title(\"Validation curve for C on the train set \")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "    sns.lineplot(data=result_df, x=\"C\", y=\"test_score\",label=\"test\")\n",
    "    plt.xscale('log')\n",
    "    plt.title(\"Validation curve for C on the test set\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "    #find the best where the test score is the highest\n",
    "    best_C=result_df[result_df[\"test_score\"]==result_df[\"test_score\"].max()][\"C\"].values[0]\n",
    "    print(\"best C:\",best_C) \n",
    "    print(\" best accuracy on test:\",result_df[\"test_score\"].max())\n",
    "    return best_C\n",
    "best_C=question_10(Cs,fold,penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- We found the best parameter in our case to be $C= 0.05$ but more values of $C$ could work as we see that the accuracy is pretty much constant over the whole set \n",
    "- Regarding the Accuracy with the TP 1, it is on the test set almost the same as with the linear regression (more or less 0.85). However the train set has an accuracy of 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11\n",
    "Reporter des indicateurs qui vous permettent de quantifier la\n",
    "qualité du modèle optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interpretation\n",
    "Suivant le cours, nous choisissons les indicateurs suivants : \n",
    "    -  Confusion Matrix \n",
    "    -  Accuracy\n",
    "    - courbe ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_11(best_C,penalty=\"l2\"):\n",
    "\n",
    "    # get the best C\n",
    "    #best_C=61\n",
    "    model=LogisticRegression(penalty=penalty,C=best_C,max_iter=1000,solver=\"liblinear\")\n",
    "    #compute the confusion matrix\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "    y_pred = cross_val_predict(model,A.drop(columns=[target]),y=A[target], cv=5)\n",
    "    conf_mat = confusion_matrix(A[target], y_pred)\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='g')\n",
    "    plt.xlabel(\"predicted\")\n",
    "    plt.ylabel(\"actual\")\n",
    "    plt.title(\" Confusion matrix for the best C\")\n",
    "\n",
    "    #Compute the classification report\n",
    "    print(classification_report(A[target], y_pred))\n",
    "\n",
    "    # compute the ROC curbs \n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    y_probas = cross_val_predict(model,A.drop(columns=[target]),y=A[target], cv=5, method=\"predict_proba\")\n",
    "    y_scores = y_probas[:, 1] # score = proba of positive class\n",
    "    fpr, tpr, thresholds = roc_curve(A[target], y_scores)\n",
    "    plt.plot(fpr, tpr, linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(\"ROC curve\")\n",
    "    plt.show()\n",
    "    # add the roc auc score \n",
    "    print(\"the area under the curve is:\",roc_auc_score(A[target], y_scores))\n",
    "    # compute the precision recall curve\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(A[target], y_scores)\n",
    "    plt.plot(recalls, precisions, linewidth=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(\"Precision recall curve\")\n",
    "    plt.show()\n",
    "    return best_C,model,fpr,tpr,recalls,precisions\n",
    "best_C,model,fpr_l2,tpr_l2,recalls_l2,precisions_l2=question_11(best_C,penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interprétation\n",
    " - the area under the roc score is close to 1, meaning that our models is good in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12 \n",
    "Reprendre les questions 9 à 11 en considérant la régression logistique régularisée l1 par rapport à l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparamets\n",
    "\n",
    "penalty=\"l1\"\n",
    "fold=5\n",
    "Cs= np.logspace(-2.3,2.3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C=question_10(Cs,fold,penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_C,model,fpr_l1,tpr_l1,recalls_l1,precisions_l1=question_11(best_C,penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both ROC curve for l1 and l2\n",
    "plt.plot(fpr_l1, tpr_l1, \"b:\", linewidth=2, label=\"l1\")\n",
    "plt.plot(fpr_l2, tpr_l2, \"r-\", linewidth=2, label=\"l2\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"the area for l1 is \",0.9219656673114119)\n",
    "print(\"the area for l2 is \",0.9222678916827852)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation:\n",
    "-Even though the ROC Curve of l1 seems visually  better, the area under both curves are quite similar, meaning that they give both same results \n",
    "-  As expected by the l1 norm, the model with l1 penalty have less non zero coefficient compared to the l2 one "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
